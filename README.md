# DocMamba: Efficient Document Pre-training with State Space Model

This repository contains an **early release** of DocMamba.  
The current version includes only a **toy version of the model encoder**, intended for reference and preliminary testing.

## üîç What's included

- A lightweight encoder module extracted from the full model
- Minimal dependencies and no training or inference scripts
- No input/output processing, loss functions, or dataset integrations yet

This version is **not intended for production or benchmarking**, but rather as a simple preview of the core architecture.


The full version of the code will be released in **a few days**.


## üìñ Citation

If you find this work useful, please consider citing our paper:

```bibtex
@inproceedings{hu2025docmamba,
  title={Docmamba: Efficient document pre-training with state space model},
  author={Hu, Pengfei and Zhang, Zhenrong and Ma, Jiefeng and Liu, Shuhang and Du, Jun and Zhang, Jianshu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={22},
  pages={24095--24103},
  year={2025}
}
